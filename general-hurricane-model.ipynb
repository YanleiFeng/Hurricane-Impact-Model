{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a wind disturbance model\n",
    "\n",
    "Use machine learning methods to build hurricane disturbance model - a general model\n",
    "\n",
    "Author: Yanlei Feng\n",
    "\n",
    "Start Date: Dec 16, 2020\n",
    "\n",
    "This documentation aims to document the steps taken to train a hurricane disturbance model, which hopefully can be used in predictions of hurricanes' impacts on on trees .\n",
    "\n",
    "## Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages and models\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import svm\n",
    "import random\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# set the random seed\n",
    "np.random.seed(1)\n",
    "\n",
    "# def all the functions used in the main code\n",
    "def cleanextreme(data):\n",
    "    \"\"\"\n",
    "    data: numpy array\n",
    "    columns: column index\n",
    "    functions:\n",
    "     1) clip DNPV to 0-1 range\n",
    "     2) remove GV extreme value above 1 or below 0\n",
    "     3) remove canopy height = 0, because the land cover map \n",
    "     used by Canopy height data were defined as non-forest, email by Marc Simard on May 26, 2020\n",
    "     \n",
    "    *Numpy array column name:\n",
    "    # 0: \"DNPV\"\n",
    "    # 1: \"preGV\"\n",
    "    # 2: \"elevation\"\n",
    "    # 3: \"aspect\"\n",
    "    # 4: \"slope\"\n",
    "    # 5: \"CH\"\n",
    "    # 6: \"precipitation\"\n",
    "    # 7: \"foresttype\"\n",
    "    # 8: \"wind\"\n",
    "    # 9: \"topodiversity\"\n",
    "    #10: \"landform\"\n",
    "    #11: \"wetness\"\n",
    "    #12: \"soilwater\"\n",
    "    #13: \"soiltexture\"\n",
    "    \"\"\"\n",
    "    data = data[(data[:,0] < 1)&(data[:,0] > 0)] \n",
    "    data = data[(data[:,1] < 1)&(data[:,1] > 0)]\n",
    "    data = data[(data[:,5] > 0)]\n",
    "    return data\n",
    "\n",
    "def digitize(k):\n",
    "    \"\"\"\n",
    "    add a column of digitized y to categorical values\n",
    "    use this digitized column to do stratified sampling\n",
    "    \"\"\"\n",
    "\n",
    "    y_numerical = k[:,0] # Split y\n",
    "    bins = np.array([0.0, .2, .4, .6, .8, 1.0])\n",
    "    y_categorical = np.digitize(y_numerical, bins)\n",
    "#     X_k = np.delete(k,0,1) # delete column_0 DNPV from data\n",
    "    X_k = np.column_stack((y_categorical, k))    \n",
    "    return X_k\n",
    "\n",
    "def rmse(A, num):\n",
    "    \"\"\"\n",
    "    select num of rows randomly from the data A\n",
    "    reduce data size to better visualize\n",
    "    \"\"\"\n",
    "    random.seed(30)\n",
    "    idx = np.random.choice(A.shape[0], num, replace= True)\n",
    "    k = A[idx]\n",
    "\n",
    "    return k\n",
    "\n",
    "#### 2. split X and Y\n",
    "def split(k):\n",
    "    \"\"\"\n",
    "    delete the first column\n",
    "    split data into X and y\n",
    "    \"\"\"\n",
    "    X_k = np.delete(k,0,1) # delete column_0 DNPV class\n",
    "    y_k = X_k[:,0] # Split y\n",
    "    X_k = np.delete(X_k,0,1) # seperate column_0 DNPV from data\n",
    "        \n",
    "    return X_k, y_k\n",
    "# just check and see the counts of each categories\n",
    "def stratify(all_hurricanes, num):\n",
    "    \"\"\"\n",
    "    hurricanes: hurricane lists or single hurricane\n",
    "    list: list of sample size of each categories\n",
    "    \"\"\"\n",
    "    all_hurricanes_bycategories = [0 for i in range(5)]\n",
    "    all_hurricanes_bycategories[0] = all_hurricanes[all_hurricanes[:,0] == 1.0]\n",
    "    all_hurricanes_bycategories[1] = all_hurricanes[all_hurricanes[:,0] == 2.0]\n",
    "    all_hurricanes_bycategories[2] = all_hurricanes[all_hurricanes[:,0] == 3.0]\n",
    "    all_hurricanes_bycategories[3] = all_hurricanes[all_hurricanes[:,0] == 4.0]\n",
    "    all_hurricanes_bycategories[4] = all_hurricanes[all_hurricanes[:,0] == 5.0]\n",
    "\n",
    "    for category in all_hurricanes_bycategories:\n",
    "        print (\"All Hurricane y categories\", category.shape)\n",
    "    all_hurricanes_bycategories_random = [0 for i in range(5)]\n",
    "    # sampling num samples from each categories\n",
    "    for index, category in enumerate(all_hurricanes_bycategories):\n",
    "        all_hurricanes_bycategories_random[index] = rmse(category, num[index])\n",
    "\n",
    "    train_all = all_hurricanes_bycategories_random[0]\n",
    "    for i in range(1,5):\n",
    "        train_all = np.concatenate((train_all, all_hurricanes_bycategories_random[i]),axis = 0)\n",
    "    return train_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "### 1. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# katrina = io.imread('Data/RegressionKatrina_07012020.tif')\n",
    "katrina = io.imread('Data/RegressionKatrina_01062021_morehigh.tif')\n",
    "rita = io.imread('Data/RegressionRita_07012020.tif')\n",
    "yasi = io.imread('Data/RegressionYasi_07012020.tif')\n",
    "maria = io.imread('Data/RegressionMaria_08282020.tif')\n",
    "laura = io.imread('Data/RegressionLaura_12142020.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_katrina_size = katrina.shape\n",
    "m_rita_size = rita.shape\n",
    "m_yasi_size = yasi.shape\n",
    "m_maria_size = maria.shape\n",
    "m_laura_size = laura.shape\n",
    "print (\"Hurricane Katrina Image Size:\" + str(m_katrina_size))\n",
    "print (\"Hurricane Rita Image Size:\" + str(m_rita_size))\n",
    "print (\"Hurricane Yasi Image Size:\" + str(m_yasi_size))\n",
    "print (\"Hurricane Maria Image Size:\" + str(m_maria_size))\n",
    "print (\"Hurricane Laura Image Size:\" + str(m_laura_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Reshape the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_katrina_flatten = katrina.reshape(-1, katrina.shape[-1])\n",
    "m_rita_flatten = rita.reshape(-1, rita.shape[-1])\n",
    "m_yasi_flatten = yasi.reshape(-1, yasi.shape[-1])\n",
    "m_maria_flatten = maria.reshape(-1, maria.shape[-1])\n",
    "m_laura_flatten = laura.reshape(-1, laura.shape[-1])\n",
    "print (\"katrina flatten shape: \" + str(m_katrina_flatten.shape))\n",
    "print (\"rita flatten shape: \" + str(m_rita_flatten.shape))\n",
    "print (\"yasi flatten shape: \" + str(m_yasi_flatten.shape))\n",
    "print (\"maria flatten shape: \" + str(m_maria_flatten.shape))\n",
    "print (\"laura flatten shape: \" + str(m_laura_flatten.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning\n",
    "### 1. Clean the NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_katrina_clean = m_katrina_flatten[~np.isnan(m_katrina_flatten).any(axis=1)]\n",
    "m_rita_clean = m_rita_flatten[~np.isnan(m_rita_flatten).any(axis=1)]\n",
    "m_yasi_clean = m_yasi_flatten[~np.isnan(m_yasi_flatten).any(axis=1)]\n",
    "m_maria_clean = m_maria_flatten[~np.isnan(m_maria_flatten).any(axis=1)]\n",
    "m_laura_clean = m_laura_flatten[~np.isnan(m_laura_flatten).any(axis=1)]\n",
    "\n",
    "print (\"katrina clean NaN shape: \" + str(m_katrina_clean.shape))\n",
    "print (\"rita clean NaN shape: \" + str(m_rita_clean.shape))\n",
    "print (\"yasi clean NaN shape: \" + str(m_yasi_clean.shape))\n",
    "print (\"maria clean NaN shape: \" + str(m_maria_clean.shape))\n",
    "print (\"laura clean NaN shape: \" + str(m_laura_clean.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Clean the extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "katrina = cleanextreme(m_katrina_clean)\n",
    "rita = cleanextreme(m_rita_clean)\n",
    "yasi = cleanextreme(m_yasi_clean)\n",
    "maria = cleanextreme(m_maria_clean)\n",
    "laura = cleanextreme(m_laura_clean)\n",
    "\n",
    "print(\"katrina clean extreme shape: \" + str(katrina.shape))\n",
    "print(\"rita clean extreme shape: \" + str(rita.shape))\n",
    "print(\"yasi clean extreme shape: \" + str(yasi.shape))\n",
    "print(\"maria clean extreme shape: \" + str(maria.shape))\n",
    "print(\"laura clean extreme shape: \" + str(laura.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. do some data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 scatter plots of wind and DNPV across all hurricanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rita_sampling = rmse(rita, 5000)\n",
    "katrina_sampling = rmse(katrina, 5000)\n",
    "yasi_sampling = rmse(yasi, 5000)\n",
    "maria_sampling = rmse(maria, 5000)\n",
    "laura_sampling = rmse(laura, 5000)\n",
    "\n",
    "fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5, sharex=True)\n",
    "fig.suptitle('Wind and disturbance')\n",
    "\n",
    "ax1.scatter(katrina_sampling[:,8],katrina_sampling[:,0], color = 'k', s = 0.5, alpha=1)\n",
    "ax1.set_xlim(19.05, 55.05)\n",
    "ax1.set_ylim(-0.05, 1.05)\n",
    "ax1.set_xlabel(\"Wind\")\n",
    "ax1.set_ylabel(\"DNPV\")\n",
    "\n",
    "ax2.scatter(rita_sampling[:,8],rita_sampling[:,0], color = 'k', s = 0.5, alpha=1)\n",
    "ax3.scatter(yasi_sampling[:,8],yasi_sampling[:,0], color = 'k', s = 0.5, alpha=1)\n",
    "ax4.scatter(maria_sampling[:,8],maria_sampling[:,0], color = 'k', s = 0.5, alpha=1) \n",
    "ax4.set_xlim(19.05, 85.05)\n",
    "ax5.scatter(laura_sampling[:,8],laura_sampling[:,0], color = 'k', s = 0.5, alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 scatter plots of wind distribution of Rita and Laura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hurricane Rita vs. Laura\n",
    "fig, (ax1, ax2) = plt.subplots(2, sharex=True)\n",
    "ax1.scatter(rita_sampling[:,8],rita_sampling[:,0], color = 'k', s = 0.5, alpha=1)\n",
    "ax1.set_title(\"Hurricane Rita\")\n",
    "ax1.set_ylabel(\"DNPV\")\n",
    "ax2.scatter(laura_sampling[:,8],laura_sampling[:,0], color = 'k', s = 0.5, alpha=1)\n",
    "ax2.set_xlim(15.05, 55.05)\n",
    "ax2.set_ylim(-0.05, 1.05)\n",
    "ax2.set_xlabel(\"Wind (m/s)\")\n",
    "ax2.set_ylabel(\"DNPV\")\n",
    "ax2.set_title(\"Hurricane Laura\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 scatter plots of precipitation distribution of Rita and Laura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precipitation\n",
    "fig, (ax1, ax2) = plt.subplots(2, sharex=True)\n",
    "ax1.scatter(rita_sampling[:,6],rita_sampling[:,0], color = 'k', s = 0.5, alpha=1)\n",
    "ax1.set_title(\"Hurricane Rita\")\n",
    "ax1.set_ylabel(\"DNPV\")\n",
    "ax2.scatter(laura_sampling[:,6],laura_sampling[:,0], color = 'k', s = 0.5, alpha=1)\n",
    "ax2.set_xlim(0.05, 210.05)\n",
    "ax2.set_ylim(-0.05, 1.05)\n",
    "ax2.set_xlabel(\"Precipitation(mm)\")\n",
    "ax2.set_ylabel(\"DNPV\")\n",
    "ax2.set_title(\"Hurricane Laura\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data preparation \n",
    "\n",
    "### 4.1 stratified sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine 4 hurricanes together\n",
    "hurricane_list_all = [katrina, rita, yasi, maria, laura]\n",
    "hurricane_name_list = [\"Katrina\", \"Rita\", \"Yasi\", \"Maria\", \"laura\"]\n",
    "\n",
    "hurricane_list_all_multiclass = [0 for i in range(len(hurricane_list_all))]\n",
    "\n",
    "# digitize y values to categorical data for stratified sampling\n",
    "for i in range(len(hurricane_list_all)):\n",
    "    random.seed(30)\n",
    "    hurricane_list_all_multiclass[i] = digitize(hurricane_list_all[i]) #store array in lists\n",
    "    np.set_printoptions(suppress=True, precision=3)\n",
    "    print(\"hurricane \" + hurricane_name_list[i] + \" size: \", hurricane_list_all_multiclass[i].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 stratified sampling and oversampling intensive disturbance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 three different hurricane combination for training data, uncomment to chosse one of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) stack all four hurricanes together\n",
    "# all_hurricanes = np.concatenate((hurricane_list_all_multiclass[0], hurricane_list_all_multiclass[1], hurricane_list_all_multiclass[2], hurricane_list_all_multiclass[3]), axis=0)\n",
    "# test_hurricane = hurricane_list_all_multiclass[4]\n",
    "\n",
    "# 2) use only katrina data\n",
    "all_hurricanes = (hurricane_list_all_multiclass[0])\n",
    "test_hurricane = hurricane_list_all_multiclass[0]\n",
    "\n",
    "# 3) train with laura and rita only\n",
    "# all_hurricanes = np.concatenate((hurricane_list_all_multiclass[4],  hurricane_list_all_multiclass[0]))\n",
    "# test_hurricane = hurricane_list_all_multiclass[1]\n",
    "\n",
    "# check the shape\n",
    "print(\"all hurricanes stack's shape\", all_hurricanes.shape)\n",
    "print(\"disturbance 0-0.2\", all_hurricanes[all_hurricanes[:,0]==1].shape)\n",
    "print(\"disturbance 0.2-0.4\", all_hurricanes[all_hurricanes[:,0]==2].shape)\n",
    "print(\"disturbance 0.4-0.6\", all_hurricanes[all_hurricanes[:,0]==3].shape)\n",
    "print(\"disturbance 0.6-0.8\", all_hurricanes[all_hurricanes[:,0]==4].shape)\n",
    "print(\"disturbance 0.8-1.0\", all_hurricanes[all_hurricanes[:,0]==5].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the y distribution to double check\n",
    "plt.hist(all_hurricanes[:,0], color='#0504aa')\n",
    "plt.title('All Hurricanes together DNPV Histogram')\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.xlabel(\"DNPV\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Different sampling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling 1\n",
    "# non-stratify random sampling\n",
    "# train_all = rmse(all_hurricanes, 50000)\n",
    "# test_all = rmse(test_hurricane, 5000)\n",
    "\n",
    "# samlpling 2\n",
    "# # stratified sampling by y categories\n",
    "# # prepare data for training and test\n",
    "# train_stratify = [10000,10000,10000,10000,10000]\n",
    "# test_stratify = [1000,1000,1000,1000,1000]\n",
    "# train_all = stratify(all_hurricanes,train_stratify)\n",
    "# test_all = stratify(test_hurricane,test_stratify)\n",
    "\n",
    "# samlpling 3\n",
    "# stratified sampling accordingly\n",
    "train_stratify_decreasing = [32000,16000,8000,4000,2000]\n",
    "test_stratify_decreasing = [3200,1600,800,400,200]\n",
    "train_all = stratify(all_hurricanes,train_stratify_decreasing)\n",
    "test_all = stratify(test_hurricane,test_stratify_decreasing)\n",
    "\n",
    "# samlpling 4\n",
    "# random sampling only intensive disturbance (upper 50%)\n",
    "p75 = np.percentile(all_hurricanes[:,1],75)\n",
    "p50 = np.median(all_hurricanes[:,1])\n",
    "print(p75, \"p75\")\n",
    "print(p50, \"p50\")\n",
    "\n",
    "# check the shape of sampled data\n",
    "print(all_hurricanes[(all_hurricanes[:,1]>0.3)].shape)\n",
    "print(test_hurricane[(test_hurricane[:,1]>0.3)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the y distribution to double check\n",
    "plt.hist(train_all[:,0], color='#0504aa')\n",
    "plt.title('All Hurricanes together DNPV Histogram')\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.xlabel(\"DNPV\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 split x and y and double check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split features and label\n",
    "X_hurricane_general_train, y_hurricane_general_train = split(train_all)\n",
    "# set up test data\n",
    "X_hurricane_general_test, y_hurricane_general_test = split(test_all)\n",
    "# split train, test\n",
    "# X_hurricane_general_train, X_hurricane_general_test, y_hurricane_general_train, y_hurricane_general_test = train_test_split(hurricane_general_X, hurricane_general_y, test_size=0.02, shuffle = True, random_state=42)\n",
    "print( \"general hurricane training data  X total\", X_hurricane_general_train.shape)\n",
    "print( \"general hurricane test data X total\", X_hurricane_general_test.shape)\n",
    "print( \"check y\", y_hurricane_general_train[:10])\n",
    "print( \"check test y\", y_hurricane_general_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot y value to double check\n",
    "plt.hist(y_hurricane_general_train, color='#0504aa', alpha=0.7, rwidth=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Feature Scaling\n",
    "The test data is unseen, so we use mean and sigma from training data to normalize test set\n",
    "\n",
    "1.) normalize the training set and save the normalization parameters<br/>\n",
    "2.) normalize the test set using the training normalization parameters\n",
    "\n",
    "#### 5.2.1 Standard normalization \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(formatter={'float_kind':'{:f}'.format})\n",
    "# standard normalization z = (X-miu)/sigma\n",
    "scaler = StandardScaler()\n",
    "    \n",
    "# for the general model\n",
    "train_scaled = scaler.fit_transform(X_hurricane_general_train)\n",
    "test_scaled = scaler.transform(X_hurricane_general_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 OneHotEncoder\n",
    "apply oneHotEncoder to the foresttype column<br/>\n",
    "Those columns specified with passthrough are added at the right to the output of the transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer([('encoder', OneHotEncoder(), [6])], remainder = 'passthrough')\n",
    "\n",
    "# for the general hurricane model\n",
    "# X_train = ct.fit_transform(train_scaled)\n",
    "# X_test_newhurr = ct.transform(test_scaled)\n",
    "\n",
    "X_train = (train_scaled)\n",
    "X_test_newhurr = (test_scaled)\n",
    "\n",
    "# After OneHotEncoder, variables become:\n",
    "    # 0-16: \"foresttype\"\n",
    "    # 17: \"preGV\"\n",
    "    # 18: \"elevation\"\n",
    "    # 19: \"aspect\"\n",
    "    # 20: \"slope\"\n",
    "    # 21: \"CH\"\n",
    "    # 22: \"precipitation\"\n",
    "    # 23: \"wind\"\n",
    "    # 24: \"topodiversity\"\n",
    "    # 25: \"landform\"\n",
    "    # 26: \"wetness\"\n",
    "    # 27: \"soilwater\"\n",
    "    # 28: \"soiltexture\"\n",
    "    \n",
    "# test log transform right skewed data\n",
    "# y_test_newhurr = np.log(y_hurricane_general_test)\n",
    "\n",
    "# no transfer on y value\n",
    "y_test_newhurr = (y_hurricane_general_test)\n",
    "y_train = (y_hurricane_general_train)\n",
    "\n",
    "# get number of features after encoding\n",
    "nrows, ncols = X_train.shape\n",
    "# for the general model\n",
    "\n",
    "print(\"Data are correctly splited and encoded, Yeah!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,  X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.33, random_state = 1)\n",
    "print( \"Training samples X: \" + str(X_train.shape) + \", y: \" + str(y_train.shape))\n",
    "print( \"Test samples X: \" + str(X_test.shape) + \", y: \" + str(y_test.shape))\n",
    "print( \"Test new hurricane samples X: \" + str(X_test_newhurr.shape) + \", y: \" + str(y_test_newhurr.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.1 Hold out test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "rf = RandomForestRegressor(n_estimators=100, max_features = \"sqrt\",random_state = 30)\n",
    "rf.fit(X_train, y_train)\n",
    "prediction_test_rf = rf.predict(X_test)\n",
    "importance_rf = rf.feature_importances_\n",
    "mse_test_rf = np.sqrt(np.mean(np.square(y_test - prediction_test_rf))) #calculate root mean square error\n",
    "corr, _ = pearsonr(y_test, prediction_test_rf)\n",
    "r2 = rf.score(X_test,y_test)\n",
    "\n",
    "\n",
    "print(\"rmse_test RF\", mse_test_rf)\n",
    "print(\"feature importance\", rf.feature_importances_)\n",
    "print('Pearsons correlation: %.3f' % corr)\n",
    "print(\"r2\", r2)\n",
    "\n",
    "print(\"training errors\")\n",
    "print(np.sqrt(np.mean(np.square(y_train-rf.predict(X_train)))))\n",
    "print(rf.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.2 Overfitting test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the model is overfitting\n",
    "# define lists to collect scores\n",
    "train_scores, test_scores = [], []\n",
    "\n",
    "# define the tree depths to evaluate\n",
    "values = [i for i in range(900, 1100, 100)]\n",
    "# evaluate a decision tree for each depth\n",
    "for i in values:\n",
    "    train_acc = 0\n",
    "    test_acc = 0\n",
    "    # configure the model\n",
    "    model = RandomForestRegressor(n_estimators=i)\n",
    "#     model = RandomForestRegressor(n_estimators=1000, random_state = 30)\n",
    "    # fit model on the training dataset\n",
    "    model.fit(X_train, y_train)\n",
    "    # evaluate on the train dataset\n",
    "    train_ypre = model.predict(X_train)\n",
    "    train_acc = np.sqrt(np.mean(np.square(y_train-train_ypre)))\n",
    "    train_scores.append(train_acc)\n",
    "    # evaluate on the test dataset\n",
    "    test_ypre = model.predict(X_test)\n",
    "    test_acc = np.sqrt(np.mean(np.square(y_test-test_ypre)))\n",
    "    test_scores.append(test_acc)\n",
    "    # summarize progress\n",
    "    print('>%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc))\n",
    "    print('r2',model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.3 Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_scores = cross_val_score(rf, X_train, y_train, scoring='neg_root_mean_squared_error', cv=4)\n",
    "print(rmse_scores)\n",
    "r2_scores = cross_val_score(rf, X_train, y_train, cv=4)\n",
    "print(r2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation average\n",
    "print(\"stratified sampling\")\n",
    "print(\"rmse mean\", np.mean(rmse_scores))\n",
    "print(\"r2 mean\", np.mean(r2_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5.5 Plot the figures between observed value and predicted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlation between prediction and true values\n",
    "plt.figure()\n",
    "plt.scatter(y_test, prediction_test_rf, color = 'k', alpha=0.5)\n",
    "plt.plot([-1, 1], [-1, 1], color = 'red', linewidth = 2)\n",
    "plt.xlim(-0.05, 1.05)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "r2 = np.round(r2, 3)\n",
    "plt.text(0.05, 0.9, r'$R^{2}$ ='+str(r2))\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Plot the feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 0: \"preGV    # 1: \"elevation\"    # 2: \"aspect\"    # 3: \"slope\"\n",
    "    # 4: \"CH\"      # 5: \"precipitation\"# 6: \"foresttype\"# 7: \"wind\"\n",
    "    # 8: \"topodiversity\"#9: \"landform\"#10: \"wetness\"   #11: \"soilwater\"\n",
    "    #12: \"soiltexture\"\n",
    "    #plot feature importance\n",
    "    # plt.bar([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25],importance_rf)\n",
    "\n",
    "\n",
    "plt.bar([0,1,2,3,4,5,6,7,8,9,10,11,12],importance_rf)\n",
    "plt.title(\" Feature Importance\")\n",
    "plt.xlabel(\"features\")\n",
    "plt.ylabel(\"importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test the hurricane model on an unseen hurricane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the general random forest model on a new hurricane\n",
    "prediction_newhurr_rf = rf.predict(X_test_newhurr)\n",
    "mse_newhurr_rf = np.sqrt(np.mean(np.square(y_test_newhurr - prediction_newhurr_rf))) #calculate root mean square error\n",
    "print(\"rmse_test RF\", mse_newhurr_rf)\n",
    "corr_newhurr, _ = pearsonr(y_test_newhurr, prediction_newhurr_rf)\n",
    "r2_newhurr = rf.score(X_test_newhurr, y_test_newhurr)\n",
    "print('Pearsons correlation: %.3f' % corr_newhurr)\n",
    "print(\"r2 new hurr\", r2_newhurr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlation between prediction and true values\n",
    "plt.figure()\n",
    "plt.scatter(test_concat[:,0], test_concat[:,1], color = 'k', alpha=0.5)\n",
    "plt.plot([0, 1], [0, 1], color = 'red', linewidth = 2)\n",
    "plt.xlim(0.05, 1.05)\n",
    "plt.ylim(0.05, 1.05)\n",
    "plt.text(0.08, 0.8, r'$R^{2}$ ='+str(round(r2_newhurr,3)))\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot test error with mean and std\n",
    "\n",
    "# calculate the test error mean and std\n",
    "test_error = test_concat[:,0]-test_concat[:,1]\n",
    "test_error_mean = np.mean(test_error)\n",
    "test_error_std = np.std(test_error)\n",
    "print(\"test_error_mean =\"+str(test_error_mean) +\"; test_error_std =\"+ str(test_error_std))\n",
    "\n",
    "# plot the error\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.hist(test_error, color = 'k', bins = 50)\n",
    "textstr = '\\n'.join((\n",
    "    r'$\\mathrm{Mean} =%.2f$' % (test_error_mean, ),\n",
    "    r'$\\mathrm{Std}=%.2f$' % (test_error_std, )))\n",
    "\n",
    "ax.text(0.05, 0.85, textstr, transform=ax.transAxes, fontsize=14,\n",
    "        verticalalignment='top')\n",
    "\n",
    "ax.set_xlabel('Prediction Errors')\n",
    "ax.set_ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
